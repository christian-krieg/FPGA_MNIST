\subsection*{Network Operations}
\subsubsection*{Convolutional Operations}

The output of an convolutional layer is defined by
\begin{equation}
   z(i,j) = (f*g)(i,j) = \sum_{m=-\infty}^{\infty} \sum_{n=-\infty}^{\infty} f(m,n) g(m-i,n-j)
\end{equation}


\subsubsection*{Fully Connected Layer}

The output of an fully connected layer is defined by
\begin{equation}
	z = xW + b
\end{equation}
where $x \in \mathbb{R}^{b,m}$,  $W \in \mathbb{R}^{m,n}$ and $b \in \mathbb{R}^{n}$. 

\subsubsection*{Rectified Linear Unit (ReLU)}

\begin{equation}
	f(x) = \begin{cases}
		x \quad \text{if} \quad x > 0 \\
		0 \quad \text{else}
	\end{cases}
\end{equation}


\subsubsection*{Softmax}
