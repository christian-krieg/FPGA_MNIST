\subsection{Convolutional Layer}

The convolutional layer - referred to as conv2d - receives an input data tensor $X_{ci} $ of size $[H, W, C_{in}]$ and outputs a data tensor of size $[H,W,C_{out}]$. Here $H$ and $W$ is the height and width of the convolutional kernel. So every input tensor is transferred to each single convolutional computation channel, which is responsible for computing a \emph{single} output channel. Each convolutional channel parameterized for with its own set of values.

\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		% Constrain the height instead of the widht so the images are equally aligned at the top
		\includesvg[height=2.5in]{img/inkscape/conv2d.svg}
		\caption[Conv2d block diagram.]{Conv2d block diagram. For each output channel a conv\_channel module is used. $k$ indicates the number of output channels.}
		\label{fig:conv2d}
	\end{subfigure}%
	~
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includesvg[height=2.5in]{img/inkscape/conv-channel.svg}
		\caption[conv\_channel block diagram.]{conv\_channel block diagram. For each input channel a kernel\_3x3 module is used. $n$ indicates the number of input channels.}
		\label{fig:conv-channel}		
	\end{subfigure}
	\caption{Block diagram of the Convolutional Layer}
	\label{fig:hw-layer-conv}
\end{figure}


Figure~\ref{fig:conv2d} shows the block diagram of a conv2d module. It uses $k$ conv\_channel modules to realise $k$ output channels. All conv\_channel modules get the same input vector $X_{c_i}$. All conv\_channel modules and the two conv2d modules are automatically generated by a Python script.

Figure \ref{fig:conv-channel} shows the block diagram of a conv\_channel module. It uses $n$ kernel\_3x3 modules to realise $n$ input channels. All kernel\_3x3 modules get a different input vector $X_{c_{i1}}$ to $X_{c_{in}}$ which are $3 \times 3$ input matrices. All kernel outputs are summed up to one final value of length BIT\_WIDTH\_OUT.

\subsubsection{Interface}

\begin{itemize}
	\item Input interface connected to shift register, which consists of a $n \cdot 3 \times 3$ vector of values of length BIT\_WIDTH\_IN, in which $n$ is the number of input channels.
	\item Output interface connected to the pooling layer, which is a vector of $m$ values of length BIT\_WIDTH\_OUT, in which $m$ is the number of output channels.
\end{itemize}
Both input and output interfaces have ready, last and valid signals to control the flow of data.

\subsubsection{Parameter}

\begin{itemize}
 	\item BIT\_WIDTH\_IN : integer
 	\item BIT\_WIDTH\_OUT : integer
 	\item INPUT\_CHANNELS: integer
 	\item OUTPUT\_CHANNELS: integer
\end{itemize}



\subsubsection*{conv\_channel}


\textbf{Interface}
\begin{itemize}
	\item Input interface, same as conv2d.
	\item Output interface connected to the pooling layer, which is a value of length BIT\_WIDTH\_OUT.
\end{itemize}

\textbf{Parameter}
\begin{itemize}
 	\item BIT\_WIDTH\_IN : integer
 	\item KERNEL\_WIDTH\_OUT : integer, output bit width of the kernel\_3x3 module
 	\item BIT\_WIDTH\_OUT: integer
 	\item N: integer, number of kernels
 	\item OUTPUT\_MSB: integer, defines which of the $n$=BIT\_WIDTH\_OUT bits is the most significant bit
 	\item BIAS: integer, currently unused as bias seems to not be very important in the convolutional layers
\end{itemize}
