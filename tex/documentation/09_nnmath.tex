\subsection{Network Operations}

\subsubsection*{Convolutional Layer}

The output of an convolutional layer is defined by
\begin{equation}
   z(i,j) = (f*g)(i,j) = \sum_{m=-\infty}^{\infty} \sum_{n=-\infty}^{\infty} f(m,n) g(m-i,n-j)
\end{equation}

It is explained in more detail here: \cite{dumoulin2016guide}

\subsubsection*{Pooling Layer}



\subsubsection*{Fully Connected Layer}

The output of an fully connected layer is defined by
\begin{equation}
	z = xW + b
\end{equation}
where $x \in \mathbb{R}^{b,m}$,  $W \in \mathbb{R}^{m,n}$ and $b \in \mathbb{R}^{n}$. 

\subsubsection*{Rectified Linear Unit (ReLU)}

\begin{equation}
	f(x) = \begin{cases}
		x \quad \text{if} \quad x > 0 \\
		0 \quad \text{else}
	\end{cases}
\end{equation}


\subsubsection*{Softmax}

For a vector $x \in \mathbb{R}^{n}$ the softmax function is defined as
\begin{equation}
	f_{\text{Softmax}}: f(x) = \frac{\exp{x_i}}{\sum_i^n \exp{x_i}}
\end{equation}